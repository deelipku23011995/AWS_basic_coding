{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "653cd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df51841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import urllib.parse\n",
    "import boto3\n",
    "import re\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import hashlib\n",
    "from botocore.errorfactory import ClientError\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2106eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "##  Extracting Daata path from s3\n",
    "\n",
    "\n",
    "bucket_name='lossadjustmentdataset'\n",
    "prefix = 'documents/'\n",
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "pages = paginator.paginate(Bucket=bucket_name,Prefix=prefix)\n",
    "ct = 0\n",
    "f_list = []\n",
    "for page in pages:\n",
    "    for obj in page['Contents']:\n",
    "        if \".pdf\" in obj['Key'].lower():\n",
    "            ct = ct + 1\n",
    "            f_list.append(obj['Key'])\n",
    "            \n",
    "print(len(f_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87ca5a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['documents/B018035480_02.pdf',\n",
       " 'documents/B0180ME1402570AA_06.pdf',\n",
       " 'documents/B0180ME1505036AA_02.pdf',\n",
       " 'documents/B0180ME1522689AA_03.pdf',\n",
       " 'documents/B0180ME1714228AB_01.pdf',\n",
       " 'documents/B0180ME1916674AA_02.pdf',\n",
       " 'documents/B0180ME1916674AA_07.pdf',\n",
       " 'documents/B0509AO1700354ACA_03.pdf',\n",
       " 'documents/B0509AO1800256ACA_03.pdf',\n",
       " 'documents/B0509AO1900218AAA_03.pdf',\n",
       " 'documents/B0509DC1600001AAA_01.pdf',\n",
       " 'documents/B0509MO4195140001_04.pdf',\n",
       " 'documents/B0509PO1600101AL_02.pdf',\n",
       " 'documents/B0509PO1600101AL_03.pdf',\n",
       " 'documents/B0509XMC008915AAA_02.pdf',\n",
       " 'documents/B0621121379_01.pdf',\n",
       " 'documents/B0621121778_01.pdf',\n",
       " 'documents/B0621173696_06.pdf',\n",
       " 'documents/B0621180015_02.pdf',\n",
       " 'documents/B0621180015_03.pdf',\n",
       " 'documents/B0621180105_02.pdf',\n",
       " 'documents/B0621180105_04.pdf',\n",
       " 'documents/B07021212011_07.pdf',\n",
       " 'documents/B07021510007_01.pdf',\n",
       " 'documents/B07021510016_02.pdf',\n",
       " 'documents/B07021512005_03.pdf',\n",
       " 'documents/B07021703006_02.pdf',\n",
       " 'documents/B07021708023_02.pdf',\n",
       " 'documents/B07021709014_03.pdf',\n",
       " 'documents/B07021802002_03.pdf',\n",
       " 'documents/B07021802012_02.pdf',\n",
       " 'documents/B07021901031_02.pdf',\n",
       " 'documents/B07021905019_03.pdf',\n",
       " 'documents/B07021907021_02.pdf',\n",
       " 'documents/B07021909022_03.pdf',\n",
       " 'documents/B0702BB301290MAF_05.pdf',\n",
       " 'documents/B0702BB301290MAF_06.pdf',\n",
       " 'documents/B0702BB301290NAB1_02.pdf',\n",
       " 'documents/B0702BB301290NAB_02.pdf',\n",
       " 'documents/B0702BB301290NAC_02.pdf',\n",
       " 'documents/B0702BB301840LAE_02.pdf',\n",
       " 'documents/B0750004766400503_03.pdf',\n",
       " 'documents/B0750004803400403_01.pdf',\n",
       " 'documents/B0750004983900402_01.pdf',\n",
       " 'documents/B0753PF1409429001_03.pdf',\n",
       " 'documents/B0753PF1410405001_02.pdf',\n",
       " 'documents/B0793MA00265A_04.pdf',\n",
       " 'documents/B080113821J17AAA_02.pdf',\n",
       " 'documents/B0823E5030117001_02.pdf',\n",
       " 'documents/B0823E5030781001_03.pdf',\n",
       " 'documents/B0901BZ1712297001_02.pdf',\n",
       " 'documents/B0901BZ1712979A01_01.pdf',\n",
       " 'documents/B0901BZ1812777001_03.pdf',\n",
       " 'documents/B0901BZ1914176001_01.pdf',\n",
       " 'documents/B0901BZ1914176001_02.pdf',\n",
       " 'documents/B0901LL1209959001_03.pdf',\n",
       " 'documents/B0901LL1415255001_01.pdf',\n",
       " 'documents/B0901LL1415255002_06.pdf',\n",
       " 'documents/B1230EU024350DA1_02.pdf',\n",
       " 'documents/B1230EU028100DA2_02.pdf',\n",
       " 'documents/B1230EU029600EA3_01.pdf',\n",
       " 'documents/B1230NG00011A15A1_06.pdf',\n",
       " 'documents/B1230NG00072A16A1_02.pdf',\n",
       " 'documents/B1230NG00370A15A1_04.pdf',\n",
       " 'documents/B1230NG00447A16A1_02.pdf',\n",
       " 'documents/B1230NG00447A16A2_01.pdf',\n",
       " 'documents/B1230NG00454A15A2_01.pdf',\n",
       " 'documents/B1230NG00503A16A1_02.pdf',\n",
       " 'documents/B1244EU006700BAB_03.pdf',\n",
       " 'documents/B1263EG0008719AAA_02.pdf',\n",
       " 'documents/B1263EG0054515AAA_03.pdf',\n",
       " 'documents/B1526001443800202_01.pdf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a78a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'documents/B0180ME1402570AA_06.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cea5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "clientt = boto3.client('comprehend')\n",
    "config = Config(retries = dict(max_attempts = 20)) # Amazon Textract client \n",
    "\n",
    "\n",
    "\n",
    "class ProcessType:\n",
    "    DETECTION = 1\n",
    "    ANALYSIS = 2\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    jobId = ''\n",
    "    textract = boto3.client('textract', config=config)\n",
    "    sqs = boto3.client('sqs')\n",
    "    sns = boto3.client('sns')\n",
    "\n",
    "    roleArn = ''\n",
    "    bucket = ''\n",
    "    document = ''\n",
    "\n",
    "    sqsQueueUrl = ''\n",
    "    snsTopicArn = ''\n",
    "    processType = ''\n",
    "    s3_dir_key = ''\n",
    "    dest_bucket = ''\n",
    "    \n",
    "    def main(self, bucketName, documentName, key_id, dest_bucket, s3_dir_key, process_type=\"DETECTION\"):\n",
    "        self.roleArn = 'arn:aws:iam::661082688832:role/service-role/AmazonSageMaker-ExecutionRole-20210921T210509'\n",
    "\n",
    "        self.bucket = bucketName\n",
    "        self.document = documentName\n",
    "        self.key_id = key_id\n",
    "        self.s3_dir_key = s3_dir_key\n",
    "        self.dest_bucket = dest_bucket\n",
    "        # self.file_name = file_name\n",
    "\n",
    "        #self.CreateTopicandQueue()\n",
    "        if process_type==\"DETECTION\":\n",
    "            self.ProcessDocument(ProcessType.DETECTION)\n",
    "        elif process_type==\"ANALYSIS\":\n",
    "            self.ProcessDocument(ProcessType.ANALYSIS)\n",
    "        else:\n",
    "            raise Exception(f\"process_type can be DETECTION/ANALYSIS, but \\\"{process_type}\\\" was passed.\")\n",
    "        #self.DeleteTopicandQueue()\n",
    "\n",
    "    def ProcessDocument(self, type):\n",
    "        jobFound = False\n",
    "\n",
    "        self.processType = type\n",
    "        validType = False\n",
    "\n",
    "        # Determine which type of processing to perform\n",
    "        if self.processType == ProcessType.DETECTION:\n",
    "            response = self.textract.start_document_text_detection(DocumentLocation={'S3Object': {'Bucket': self.bucket, 'Name': self.document}})\n",
    "            print('Processing type: Detection')\n",
    "            validType = True\n",
    "\n",
    "        if self.processType == ProcessType.ANALYSIS:\n",
    "            response = self.textract.start_document_analysis(DocumentLocation={'S3Object': {'Bucket': self.bucket, 'Name': self.document}},\n",
    "                                                             FeatureTypes=[\n",
    "                                                                 \"FORMS\",\"LAYOUT\"])\n",
    "            print('Processing type: Analysis')\n",
    "            validType = True\n",
    "\n",
    "        if validType == False:\n",
    "            raise Exception(f\"process_type can be DETECTION/ANALYSIS, but \\\"{process_type}\\\" was passed.\")\n",
    "\n",
    "        print('Start Job Id: ' + response['JobId'])\n",
    "        \n",
    "        \n",
    "        if self.processType == ProcessType.DETECTION:\n",
    "            while(self.textract.get_document_text_detection(JobId=str(response['JobId']))[\"JobStatus\"]!=\"SUCCEEDED\"):\n",
    "                pass\n",
    "        \n",
    "        if self.processType == ProcessType.ANALYSIS:\n",
    "            while(self.textract.get_document_analysis(JobId=str(response['JobId']))[\"JobStatus\"]!=\"SUCCEEDED\"):\n",
    "                pass\n",
    "        \n",
    "        print('Matching Job Found:' + response['JobId'])\n",
    "        print(\"storing in S3\")\n",
    "        jobFound = True\n",
    "        results = self.GetResults(response['JobId'])\n",
    "        self.StoreInS3(results)\n",
    "\n",
    "\n",
    "        print('Done!')\n",
    "\n",
    "    # Store the result in a S3 bucket\n",
    "    def StoreInS3(self, response):\n",
    "        print('registering in s3 bucket...')\n",
    "        outputInJsonText = str(response)\n",
    "        filename = str(self.key_id).split('/')[-1]\n",
    "        pdfTextExtractionS3ObjectName = os.path.join(self.s3_dir_key, str(filename) + \".json\") \n",
    "        pdfTextExtractionS3Bucket = self.dest_bucket\n",
    "\n",
    "        s3 = boto3.client('s3')\n",
    "\n",
    "        s3.put_object(Body=outputInJsonText,\n",
    "                      Bucket= pdfTextExtractionS3Bucket, Key=pdfTextExtractionS3ObjectName)\n",
    "        print('file ' + pdfTextExtractionS3ObjectName + ' registered successfully!')\n",
    "\n",
    "    def CreateTopicandQueue(self):\n",
    "\n",
    "        millis = str(int(round(time.time() * 1000)))\n",
    "\n",
    "        # Create SNS topic\n",
    "        snsTopicName = \"AmazonTextractTopic\" + millis\n",
    "\n",
    "        topicResponse = self.sns.create_topic(Name=snsTopicName)\n",
    "        self.snsTopicArn = topicResponse['TopicArn']\n",
    "\n",
    "        # create SQS queue\n",
    "        sqsQueueName = \"AmazonTextractQueue\" + millis\n",
    "        self.sqs.create_queue(QueueName=sqsQueueName)\n",
    "        self.sqsQueueUrl = self.sqs.get_queue_url(\n",
    "            QueueName=sqsQueueName)['QueueUrl']\n",
    "\n",
    "        attribs = self.sqs.get_queue_attributes(QueueUrl=self.sqsQueueUrl,\n",
    "                                                AttributeNames=['QueueArn'])['Attributes']\n",
    "\n",
    "        sqsQueueArn = attribs['QueueArn']\n",
    "\n",
    "        # Subscribe SQS queue to SNS topic\n",
    "        self.sns.subscribe(\n",
    "            TopicArn=self.snsTopicArn,\n",
    "            Protocol='sqs',\n",
    "            Endpoint=sqsQueueArn)\n",
    "\n",
    "        # Authorize SNS to write SQS queue\n",
    "        policy = \"\"\"{{\n",
    "  \"Version\":\"2012-10-17\",\n",
    "  \"Statement\":[\n",
    "    {{\n",
    "      \"Sid\":\"MyPolicy\",\n",
    "      \"Effect\":\"Allow\",\n",
    "      \"Principal\" : {{\"AWS\" : \"*\"}},\n",
    "      \"Action\":\"SQS:SendMessage\",\n",
    "      \"Resource\": \"{}\",\n",
    "      \"Condition\":{{\n",
    "        \"ArnEquals\":{{\n",
    "          \"aws:SourceArn\": \"{}\"\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "  ]\n",
    "}}\"\"\".format(sqsQueueArn, self.snsTopicArn)\n",
    "\n",
    "        response = self.sqs.set_queue_attributes(\n",
    "            QueueUrl=self.sqsQueueUrl,\n",
    "            Attributes={\n",
    "                'Policy': policy\n",
    "            })\n",
    "\n",
    "    def DeleteTopicandQueue(self):\n",
    "        self.sqs.delete_queue(QueueUrl=self.sqsQueueUrl)\n",
    "        self.sns.delete_topic(TopicArn=self.snsTopicArn)\n",
    "\n",
    "    def GetResults(self, jobId):\n",
    "        maxResults = 1000\n",
    "        paginationToken = None\n",
    "        finished = False\n",
    "        pages = []\n",
    "\n",
    "        while finished == False:\n",
    "\n",
    "            response = None\n",
    "\n",
    "            if self.processType == ProcessType.DETECTION:\n",
    "                if paginationToken == None:\n",
    "                    response = self.textract.get_document_text_detection(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults)\n",
    "                else:\n",
    "                    response = self.textract.get_document_text_detection(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults,\n",
    "                                                                         NextToken=paginationToken)\n",
    "                    \n",
    "                    \n",
    "            if self.processType == ProcessType.ANALYSIS:\n",
    "                if paginationToken == None:\n",
    "                    response = self.textract.get_document_analysis(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults)\n",
    "                else:\n",
    "                    response = self.textract.get_document_analysis(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults,\n",
    "                                                                         NextToken=paginationToken)\n",
    "\n",
    "\n",
    "            # Put response on pages list\n",
    "            pages.append(response)\n",
    "\n",
    "            if 'NextToken' in response:\n",
    "                paginationToken = response['NextToken']\n",
    "            else:\n",
    "                finished = True\n",
    "\n",
    "        # convert pages as JSON pattern\n",
    "        line_list=[]\n",
    "        word_counter = 0\n",
    "        line_counter = 0\n",
    "        n_pages = (pages[0][\"DocumentMetadata\"][\"Pages\"])\n",
    "  \n",
    "        for item in pages[0][\"Blocks\"]:\n",
    "            if item[\"BlockType\"] == \"LINE\":\n",
    "                line_counter = line_counter + 1\n",
    "                line_list.append(item[\"Text\"])\n",
    "            if item[\"BlockType\"] == \"WORD\":\n",
    "                word_counter = word_counter + 1\n",
    "                \n",
    "        rawtext=' '.join(line_list)\n",
    "        cutoff = min(500,len(rawtext))\n",
    "        language = \"EN\"\n",
    "        # response = clientt.detect_dominant_language(Text=str(rawtext)[:cutoff])\n",
    "        # language = response[\"Languages\"][0][\"LanguageCode\"]\n",
    "\n",
    "        pages = json.dumps(pages)\n",
    "        return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3167329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_analysis(bucket, document):\n",
    "    #Get the document from S3\n",
    "    s3_connection = boto3.resource(\"s3\")\n",
    "    \n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    result = client.get_object(Bucket=bucket, Key=document)\n",
    "    text = result['Body'].read().decode('utf-8')\n",
    "    res = json.loads(text)\n",
    "    \n",
    "    left_cor = []\n",
    "    top_cor = []\n",
    "    width_cor = []\n",
    "    height_cor = []\n",
    "    page = []\n",
    "\n",
    "    line_text = []\n",
    "\n",
    "    for response in res:\n",
    "        blocks=response[\"Blocks\"]\n",
    "        for block in blocks:\n",
    "            if (block[\"BlockType\"] == \"WORD\"):\n",
    "                left_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Left\"])))\n",
    "                top_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Top\"])))\n",
    "                width_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Width\"])))\n",
    "                height_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Height\"])))\n",
    "                line_text.append((block[\"Text\"]))\n",
    "                page.append(block[\"Page\"])\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(left_cor,top_cor,width_cor,height_cor,line_text,page)),columns = [\"xmin\",\"ymin\",\"width_cor\",\"height_cor\",\"line_text\",\"page\"])    \n",
    "    df[\"xmax\"] = (df[\"xmin\"] + df[\"width_cor\"])\n",
    "    df[\"ymax\"] = (df[\"ymin\"] + df[\"height_cor\"])\n",
    "    \n",
    "    pages = df.page.unique().tolist()\n",
    "    text_dict = {}\n",
    "    for p in pages:\n",
    "        dfp = df[df.page==p]\n",
    "        txt_list = dfp.line_text.tolist()\n",
    "        txt = \" \".join(txt_list)\n",
    "        text_dict[p] = txt\n",
    "\n",
    "    return df,text_dict,res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cc2f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_path = 'documents/B0180ME1402570AA_06.pdf'\n",
    "# key = key_path\n",
    "# s3 = boto3.client('s3')\n",
    "# bucket ='lossadjustmentdataset'\n",
    "# dest_bucket = 'deeliptutorial'\n",
    "# s3_dir_key = 'XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut'\n",
    "\n",
    "# analyzer = DocumentProcessor()\n",
    "# digest = key.replace(\".pdf\",\"\")\n",
    "# analyzer.main(bucket,key_path,digest, dest_bucket, s3_dir_key, process_type=\"ANALYSIS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd5ae750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut/B0180ME1402570AA_06.json']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Extracting Daata path from s3\n",
    "\n",
    "\n",
    "bucket_name='deeliptutorial'\n",
    "prefix = 'XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut'\n",
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "pages = paginator.paginate(Bucket=bucket_name,Prefix=prefix)\n",
    "ct = 0\n",
    "f_list = []\n",
    "for page in pages:\n",
    "    for obj in page['Contents']:\n",
    "        if \".json\" in obj['Key'].lower():\n",
    "            ct = ct + 1\n",
    "            f_list.append(obj['Key'])\n",
    "            \n",
    "print(len(f_list))\n",
    "\n",
    "f_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "451241ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_path = 'XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut/B0180ME1402570AA_06.json'\n",
    "\n",
    "bucket_name='deeliptutorial'\n",
    "df,text_dict,res = process_text_analysis(bucket_name, key_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e398f1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.Object(bucket_name='deeliptutorial', key='XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut/data.xlsx')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Saving the dataframe in the form of excel \\\n",
    "\n",
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "# ...\n",
    "\n",
    "# make data frame 'df'\n",
    "\n",
    "with io.BytesIO() as output:\n",
    "    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "        df.to_excel(writer)\n",
    "    data = output.getvalue()\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket('deeliptutorial').put_object(Key='XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut/data.xlsx', Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f23dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6946e34c",
   "metadata": {},
   "source": [
    "### Complete Pipeline for Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4bb3894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import urllib.parse\n",
    "import boto3\n",
    "import re\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import hashlib\n",
    "from botocore.errorfactory import ClientError\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import argparse\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "clientt = boto3.client('comprehend')\n",
    "config = Config(retries = dict(max_attempts = 20)) # Amazon Textract client \n",
    "\n",
    "\n",
    "\n",
    "class ProcessType:\n",
    "    DETECTION = 1\n",
    "    ANALYSIS = 2\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    jobId = ''\n",
    "    textract = boto3.client('textract', config=config)\n",
    "    sqs = boto3.client('sqs')\n",
    "    sns = boto3.client('sns')\n",
    "\n",
    "    roleArn = ''\n",
    "    bucket = ''\n",
    "    document = ''\n",
    "\n",
    "    sqsQueueUrl = ''\n",
    "    snsTopicArn = ''\n",
    "    processType = ''\n",
    "    s3_dir_key = ''\n",
    "    dest_bucket = ''\n",
    "    \n",
    "    def main(self, bucketName, documentName, key_id, dest_bucket, s3_dir_key, process_type=\"DETECTION\"):\n",
    "        self.roleArn = 'arn:aws:iam::661082688832:role/service-role/AmazonSageMaker-ExecutionRole-20210921T210509'\n",
    "\n",
    "        self.bucket = bucketName\n",
    "        self.document = documentName\n",
    "        self.key_id = key_id\n",
    "        self.s3_dir_key = s3_dir_key\n",
    "        self.dest_bucket = dest_bucket\n",
    "        # self.file_name = file_name\n",
    "\n",
    "        #self.CreateTopicandQueue()\n",
    "        if process_type==\"DETECTION\":\n",
    "            self.ProcessDocument(ProcessType.DETECTION)\n",
    "        elif process_type==\"ANALYSIS\":\n",
    "            self.ProcessDocument(ProcessType.ANALYSIS)\n",
    "        else:\n",
    "            raise Exception(f\"process_type can be DETECTION/ANALYSIS, but \\\"{process_type}\\\" was passed.\")\n",
    "        #self.DeleteTopicandQueue()\n",
    "\n",
    "    def ProcessDocument(self, type):\n",
    "        jobFound = False\n",
    "\n",
    "        self.processType = type\n",
    "        validType = False\n",
    "\n",
    "        # Determine which type of processing to perform\n",
    "        if self.processType == ProcessType.DETECTION:\n",
    "            response = self.textract.start_document_text_detection(DocumentLocation={'S3Object': {'Bucket': self.bucket, 'Name': self.document}})\n",
    "            print('Processing type: Detection')\n",
    "            validType = True\n",
    "\n",
    "        if self.processType == ProcessType.ANALYSIS:\n",
    "            response = self.textract.start_document_analysis(DocumentLocation={'S3Object': {'Bucket': self.bucket, 'Name': self.document}},\n",
    "                                                             FeatureTypes=[\n",
    "                                                                 \"FORMS\",\"LAYOUT\"])\n",
    "            print('Processing type: Analysis')\n",
    "            validType = True\n",
    "\n",
    "        if validType == False:\n",
    "            raise Exception(f\"process_type can be DETECTION/ANALYSIS, but \\\"{process_type}\\\" was passed.\")\n",
    "\n",
    "        print('Start Job Id: ' + response['JobId'])\n",
    "        \n",
    "        \n",
    "        if self.processType == ProcessType.DETECTION:\n",
    "            while(self.textract.get_document_text_detection(JobId=str(response['JobId']))[\"JobStatus\"]!=\"SUCCEEDED\"):\n",
    "                pass\n",
    "        \n",
    "        if self.processType == ProcessType.ANALYSIS:\n",
    "            while(self.textract.get_document_analysis(JobId=str(response['JobId']))[\"JobStatus\"]!=\"SUCCEEDED\"):\n",
    "                pass\n",
    "        \n",
    "        print('Matching Job Found:' + response['JobId'])\n",
    "        print(\"storing in S3\")\n",
    "        jobFound = True\n",
    "        results = self.GetResults(response['JobId'])\n",
    "        self.StoreInS3(results)\n",
    "\n",
    "\n",
    "        print('Done!')\n",
    "\n",
    "    # Store the result in a S3 bucket\n",
    "    def StoreInS3(self, response):\n",
    "        print('registering in s3 bucket...')\n",
    "        outputInJsonText = str(response)\n",
    "        filename = str(self.key_id).split('/')[-1]\n",
    "        pdfTextExtractionS3ObjectName = os.path.join(self.s3_dir_key, str(filename) + \".json\") \n",
    "        pdfTextExtractionS3Bucket = self.dest_bucket\n",
    "\n",
    "        s3 = boto3.client('s3')\n",
    "\n",
    "        s3.put_object(Body=outputInJsonText,\n",
    "                      Bucket= pdfTextExtractionS3Bucket, Key=pdfTextExtractionS3ObjectName)\n",
    "        print('file ' + pdfTextExtractionS3ObjectName + ' registered successfully!')\n",
    "\n",
    "    def CreateTopicandQueue(self):\n",
    "\n",
    "        millis = str(int(round(time.time() * 1000)))\n",
    "\n",
    "        # Create SNS topic\n",
    "        snsTopicName = \"AmazonTextractTopic\" + millis\n",
    "\n",
    "        topicResponse = self.sns.create_topic(Name=snsTopicName)\n",
    "        self.snsTopicArn = topicResponse['TopicArn']\n",
    "\n",
    "        # create SQS queue\n",
    "        sqsQueueName = \"AmazonTextractQueue\" + millis\n",
    "        self.sqs.create_queue(QueueName=sqsQueueName)\n",
    "        self.sqsQueueUrl = self.sqs.get_queue_url(\n",
    "            QueueName=sqsQueueName)['QueueUrl']\n",
    "\n",
    "        attribs = self.sqs.get_queue_attributes(QueueUrl=self.sqsQueueUrl,\n",
    "                                                AttributeNames=['QueueArn'])['Attributes']\n",
    "\n",
    "        sqsQueueArn = attribs['QueueArn']\n",
    "\n",
    "        # Subscribe SQS queue to SNS topic\n",
    "        self.sns.subscribe(\n",
    "            TopicArn=self.snsTopicArn,\n",
    "            Protocol='sqs',\n",
    "            Endpoint=sqsQueueArn)\n",
    "\n",
    "        # Authorize SNS to write SQS queue\n",
    "        policy = \"\"\"{{\n",
    "  \"Version\":\"2012-10-17\",\n",
    "  \"Statement\":[\n",
    "    {{\n",
    "      \"Sid\":\"MyPolicy\",\n",
    "      \"Effect\":\"Allow\",\n",
    "      \"Principal\" : {{\"AWS\" : \"*\"}},\n",
    "      \"Action\":\"SQS:SendMessage\",\n",
    "      \"Resource\": \"{}\",\n",
    "      \"Condition\":{{\n",
    "        \"ArnEquals\":{{\n",
    "          \"aws:SourceArn\": \"{}\"\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "  ]\n",
    "}}\"\"\".format(sqsQueueArn, self.snsTopicArn)\n",
    "\n",
    "        response = self.sqs.set_queue_attributes(\n",
    "            QueueUrl=self.sqsQueueUrl,\n",
    "            Attributes={\n",
    "                'Policy': policy\n",
    "            })\n",
    "\n",
    "    def DeleteTopicandQueue(self):\n",
    "        self.sqs.delete_queue(QueueUrl=self.sqsQueueUrl)\n",
    "        self.sns.delete_topic(TopicArn=self.snsTopicArn)\n",
    "\n",
    "    def GetResults(self, jobId):\n",
    "        maxResults = 1000\n",
    "        paginationToken = None\n",
    "        finished = False\n",
    "        pages = []\n",
    "\n",
    "        while finished == False:\n",
    "\n",
    "            response = None\n",
    "\n",
    "            if self.processType == ProcessType.DETECTION:\n",
    "                if paginationToken == None:\n",
    "                    response = self.textract.get_document_text_detection(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults)\n",
    "                else:\n",
    "                    response = self.textract.get_document_text_detection(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults,\n",
    "                                                                         NextToken=paginationToken)\n",
    "                    \n",
    "                    \n",
    "            if self.processType == ProcessType.ANALYSIS:\n",
    "                if paginationToken == None:\n",
    "                    response = self.textract.get_document_analysis(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults)\n",
    "                else:\n",
    "                    response = self.textract.get_document_analysis(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults,\n",
    "                                                                         NextToken=paginationToken)\n",
    "\n",
    "\n",
    "            # Put response on pages list\n",
    "            pages.append(response)\n",
    "\n",
    "            if 'NextToken' in response:\n",
    "                paginationToken = response['NextToken']\n",
    "            else:\n",
    "                finished = True\n",
    "\n",
    "        # convert pages as JSON pattern\n",
    "        line_list=[]\n",
    "        word_counter = 0\n",
    "        line_counter = 0\n",
    "        n_pages = (pages[0][\"DocumentMetadata\"][\"Pages\"])\n",
    "  \n",
    "        for item in pages[0][\"Blocks\"]:\n",
    "            if item[\"BlockType\"] == \"LINE\":\n",
    "                line_counter = line_counter + 1\n",
    "                line_list.append(item[\"Text\"])\n",
    "            if item[\"BlockType\"] == \"WORD\":\n",
    "                word_counter = word_counter + 1\n",
    "                \n",
    "        rawtext=' '.join(line_list)\n",
    "        cutoff = min(500,len(rawtext))\n",
    "        language = \"EN\"\n",
    "        # response = clientt.detect_dominant_language(Text=str(rawtext)[:cutoff])\n",
    "        # language = response[\"Languages\"][0][\"LanguageCode\"]\n",
    "\n",
    "        pages = json.dumps(pages)\n",
    "        return pages\n",
    "    \n",
    "    \n",
    "    \n",
    "def process_text_analysis(bucket, document):\n",
    "    #Get the document from S3\n",
    "    s3_connection = boto3.resource(\"s3\")\n",
    "    \n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    result = client.get_object(Bucket=bucket, Key=document)\n",
    "    text = result['Body'].read().decode('utf-8')\n",
    "    res = json.loads(text)\n",
    "    \n",
    "    left_cor = []\n",
    "    top_cor = []\n",
    "    width_cor = []\n",
    "    height_cor = []\n",
    "    page = []\n",
    "\n",
    "    line_text = []\n",
    "\n",
    "    for response in res:\n",
    "        blocks=response[\"Blocks\"]\n",
    "        for block in blocks:\n",
    "            if (block[\"BlockType\"] == \"WORD\"):\n",
    "                left_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Left\"])))\n",
    "                top_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Top\"])))\n",
    "                width_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Width\"])))\n",
    "                height_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Height\"])))\n",
    "                line_text.append((block[\"Text\"]))\n",
    "                page.append(block[\"Page\"])\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(left_cor,top_cor,width_cor,height_cor,line_text,page)),columns = [\"xmin\",\"ymin\",\"width_cor\",\"height_cor\",\"line_text\",\"page\"])    \n",
    "    df[\"xmax\"] = (df[\"xmin\"] + df[\"width_cor\"])\n",
    "    df[\"ymax\"] = (df[\"ymin\"] + df[\"height_cor\"])\n",
    "    \n",
    "    pages = df.page.unique().tolist()\n",
    "    text_dict = {}\n",
    "    for p in pages:\n",
    "        dfp = df[df.page==p]\n",
    "        txt_list = dfp.line_text.tolist()\n",
    "        txt = \" \".join(txt_list)\n",
    "        text_dict[p] = txt\n",
    "\n",
    "    return df,text_dict,res\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"key_path\", type=str)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(\"Received arguments {}\".format(args))\n",
    "    print(\"Key Path:---\",args.key_path)\n",
    "    \n",
    "\n",
    "    key_path = args.key_path\n",
    "    key = key_path\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket ='lossadjustmentdataset'\n",
    "    dest_bucket = 'deeliptutorial'\n",
    "    s3_dir_key = 'XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut'\n",
    "\n",
    "    analyzer = DocumentProcessor()\n",
    "    digest = key.replace(\".pdf\",\"\")\n",
    "    analyzer.main(bucket,key_path,digest, dest_bucket, s3_dir_key, process_type=\"ANALYSIS\")\n",
    "\n",
    "\n",
    "    #  Extracting Daata path from s3\n",
    "\n",
    "\n",
    "    bucket_name='deeliptutorial'\n",
    "    prefix = 'XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut'\n",
    "\n",
    "    import boto3\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name,Prefix=prefix)\n",
    "    ct = 0\n",
    "    f_list = []\n",
    "    for page in pages:\n",
    "        for obj in page['Contents']:\n",
    "            if \".json\" in obj['Key'].lower():\n",
    "                ct = ct + 1\n",
    "                f_list.append(obj['Key'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "df118bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: preprocessing.py [-h] key_path\n",
      "\n",
      "positional arguments:\n",
      "  key_path\n",
      "\n",
      "options:\n",
      "  -h, --help  show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "%run preprocessing.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f25e077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received arguments Namespace(key_path='documents/B0180ME1402570AA_06.pdf')\n",
      "Key Path:--- documents/B0180ME1402570AA_06.pdf\n",
      "Processing type: Analysis\n",
      "Start Job Id: 809d6769534918e235fe6c0fe1c3da0e955be43296de6833f246b810ee3c6594\n",
      "Matching Job Found:809d6769534918e235fe6c0fe1c3da0e955be43296de6833f246b810ee3c6594\n",
      "storing in S3\n",
      "registering in s3 bucket...\n",
      "file XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut/B0180ME1402570AA_06.json registered successfully!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "%run preprocessing.py 'documents/B0180ME1402570AA_06.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbf195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87476fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c85e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
