{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f8fd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\", role=role, instance_type=\"ml.m5.xlarge\", instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b22970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import urllib.parse\n",
    "import boto3\n",
    "import re\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import hashlib\n",
    "from botocore.errorfactory import ClientError\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import argparse\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "config = Config(retries = dict(max_attempts = 20),region_name='eu-west-2') # Amazon Textract client \n",
    "\n",
    "\n",
    "\n",
    "class ProcessType:\n",
    "    DETECTION = 1\n",
    "    ANALYSIS = 2\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    jobId = ''\n",
    "    textract = boto3.client('textract', config=config)\n",
    "    sqs = boto3.client('sqs',config=config)\n",
    "    sns = boto3.client('sns',config=config)\n",
    "\n",
    "    roleArn = ''\n",
    "    bucket = ''\n",
    "    document = ''\n",
    "\n",
    "    sqsQueueUrl = ''\n",
    "    snsTopicArn = ''\n",
    "    processType = ''\n",
    "    s3_dir_key = ''\n",
    "    dest_bucket = ''\n",
    "    \n",
    "    def main(self, bucketName, documentName, key_id, dest_bucket, s3_dir_key, process_type=\"DETECTION\"):\n",
    "        self.roleArn = 'arn:aws:iam::661082688832:role/service-role/AmazonSageMaker-ExecutionRole-20210921T210509'\n",
    "\n",
    "        self.bucket = bucketName\n",
    "        self.document = documentName\n",
    "        self.key_id = key_id\n",
    "        self.s3_dir_key = s3_dir_key\n",
    "        self.dest_bucket = dest_bucket\n",
    "        # self.file_name = file_name\n",
    "\n",
    "        #self.CreateTopicandQueue()\n",
    "        if process_type==\"DETECTION\":\n",
    "            self.ProcessDocument(ProcessType.DETECTION)\n",
    "        elif process_type==\"ANALYSIS\":\n",
    "            self.ProcessDocument(ProcessType.ANALYSIS)\n",
    "        else:\n",
    "            raise Exception(f\"process_type can be DETECTION/ANALYSIS, but \\\"{process_type}\\\" was passed.\")\n",
    "        #self.DeleteTopicandQueue()\n",
    "\n",
    "    def ProcessDocument(self, type):\n",
    "        jobFound = False\n",
    "\n",
    "        self.processType = type\n",
    "        validType = False\n",
    "\n",
    "        # Determine which type of processing to perform\n",
    "        if self.processType == ProcessType.DETECTION:\n",
    "            response = self.textract.start_document_text_detection(DocumentLocation={'S3Object': {'Bucket': self.bucket, 'Name': self.document}})\n",
    "            print('Processing type: Detection')\n",
    "            validType = True\n",
    "\n",
    "        if self.processType == ProcessType.ANALYSIS:\n",
    "            response = self.textract.start_document_analysis(DocumentLocation={'S3Object': {'Bucket': self.bucket, 'Name': self.document}},\n",
    "                                                             FeatureTypes=[\n",
    "                                                                 \"FORMS\",\"LAYOUT\"])\n",
    "            print('Processing type: Analysis')\n",
    "            validType = True\n",
    "\n",
    "        if validType == False:\n",
    "            raise Exception(f\"process_type can be DETECTION/ANALYSIS, but \\\"{process_type}\\\" was passed.\")\n",
    "\n",
    "        print('Start Job Id: ' + response['JobId'])\n",
    "        \n",
    "        \n",
    "        if self.processType == ProcessType.DETECTION:\n",
    "            while(self.textract.get_document_text_detection(JobId=str(response['JobId']))[\"JobStatus\"]!=\"SUCCEEDED\"):\n",
    "                pass\n",
    "        \n",
    "        if self.processType == ProcessType.ANALYSIS:\n",
    "            while(self.textract.get_document_analysis(JobId=str(response['JobId']))[\"JobStatus\"]!=\"SUCCEEDED\"):\n",
    "                pass\n",
    "        \n",
    "        print('Matching Job Found:' + response['JobId'])\n",
    "        print(\"storing in S3\")\n",
    "        jobFound = True\n",
    "        results = self.GetResults(response['JobId'])\n",
    "        self.StoreInS3(results)\n",
    "\n",
    "\n",
    "        print('Done!')\n",
    "\n",
    "    # Store the result in a S3 bucket\n",
    "    def StoreInS3(self, response):\n",
    "        print('registering in s3 bucket...')\n",
    "        outputInJsonText = str(response)\n",
    "        filename = str(self.key_id).split('/')[-1]\n",
    "        pdfTextExtractionS3ObjectName = os.path.join(self.s3_dir_key, str(filename) + \".json\") \n",
    "        pdfTextExtractionS3Bucket = self.dest_bucket\n",
    "\n",
    "        s3 = boto3.client('s3')\n",
    "\n",
    "        s3.put_object(Body=outputInJsonText,\n",
    "                      Bucket= pdfTextExtractionS3Bucket, Key=pdfTextExtractionS3ObjectName)\n",
    "        print('file ' + pdfTextExtractionS3ObjectName + ' registered successfully!')\n",
    "\n",
    "    def CreateTopicandQueue(self):\n",
    "\n",
    "        millis = str(int(round(time.time() * 1000)))\n",
    "\n",
    "        # Create SNS topic\n",
    "        snsTopicName = \"AmazonTextractTopic\" + millis\n",
    "\n",
    "        topicResponse = self.sns.create_topic(Name=snsTopicName)\n",
    "        self.snsTopicArn = topicResponse['TopicArn']\n",
    "\n",
    "        # create SQS queue\n",
    "        sqsQueueName = \"AmazonTextractQueue\" + millis\n",
    "        self.sqs.create_queue(QueueName=sqsQueueName)\n",
    "        self.sqsQueueUrl = self.sqs.get_queue_url(\n",
    "            QueueName=sqsQueueName)['QueueUrl']\n",
    "\n",
    "        attribs = self.sqs.get_queue_attributes(QueueUrl=self.sqsQueueUrl,\n",
    "                                                AttributeNames=['QueueArn'])['Attributes']\n",
    "\n",
    "        sqsQueueArn = attribs['QueueArn']\n",
    "\n",
    "        # Subscribe SQS queue to SNS topic\n",
    "        self.sns.subscribe(\n",
    "            TopicArn=self.snsTopicArn,\n",
    "            Protocol='sqs',\n",
    "            Endpoint=sqsQueueArn)\n",
    "\n",
    "        # Authorize SNS to write SQS queue\n",
    "        policy = \"\"\"{{\n",
    "  \"Version\":\"2012-10-17\",\n",
    "  \"Statement\":[\n",
    "    {{\n",
    "      \"Sid\":\"MyPolicy\",\n",
    "      \"Effect\":\"Allow\",\n",
    "      \"Principal\" : {{\"AWS\" : \"*\"}},\n",
    "      \"Action\":\"SQS:SendMessage\",\n",
    "      \"Resource\": \"{}\",\n",
    "      \"Condition\":{{\n",
    "        \"ArnEquals\":{{\n",
    "          \"aws:SourceArn\": \"{}\"\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "  ]\n",
    "}}\"\"\".format(sqsQueueArn, self.snsTopicArn)\n",
    "\n",
    "        response = self.sqs.set_queue_attributes(\n",
    "            QueueUrl=self.sqsQueueUrl,\n",
    "            Attributes={\n",
    "                'Policy': policy\n",
    "            })\n",
    "\n",
    "    def DeleteTopicandQueue(self):\n",
    "        self.sqs.delete_queue(QueueUrl=self.sqsQueueUrl)\n",
    "        self.sns.delete_topic(TopicArn=self.snsTopicArn)\n",
    "\n",
    "    def GetResults(self, jobId):\n",
    "        maxResults = 1000\n",
    "        paginationToken = None\n",
    "        finished = False\n",
    "        pages = []\n",
    "\n",
    "        while finished == False:\n",
    "\n",
    "            response = None\n",
    "\n",
    "            if self.processType == ProcessType.DETECTION:\n",
    "                if paginationToken == None:\n",
    "                    response = self.textract.get_document_text_detection(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults)\n",
    "                else:\n",
    "                    response = self.textract.get_document_text_detection(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults,\n",
    "                                                                         NextToken=paginationToken)\n",
    "                    \n",
    "                    \n",
    "            if self.processType == ProcessType.ANALYSIS:\n",
    "                if paginationToken == None:\n",
    "                    response = self.textract.get_document_analysis(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults)\n",
    "                else:\n",
    "                    response = self.textract.get_document_analysis(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults,\n",
    "                                                                         NextToken=paginationToken)\n",
    "\n",
    "\n",
    "            # Put response on pages list\n",
    "            pages.append(response)\n",
    "\n",
    "            if 'NextToken' in response:\n",
    "                paginationToken = response['NextToken']\n",
    "            else:\n",
    "                finished = True\n",
    "\n",
    "        # convert pages as JSON pattern\n",
    "        line_list=[]\n",
    "        word_counter = 0\n",
    "        line_counter = 0\n",
    "        n_pages = (pages[0][\"DocumentMetadata\"][\"Pages\"])\n",
    "  \n",
    "        for item in pages[0][\"Blocks\"]:\n",
    "            if item[\"BlockType\"] == \"LINE\":\n",
    "                line_counter = line_counter + 1\n",
    "                line_list.append(item[\"Text\"])\n",
    "            if item[\"BlockType\"] == \"WORD\":\n",
    "                word_counter = word_counter + 1\n",
    "                \n",
    "        rawtext=' '.join(line_list)\n",
    "        cutoff = min(500,len(rawtext))\n",
    "        language = \"EN\"\n",
    "        # response = clientt.detect_dominant_language(Text=str(rawtext)[:cutoff])\n",
    "        # language = response[\"Languages\"][0][\"LanguageCode\"]\n",
    "\n",
    "        pages = json.dumps(pages)\n",
    "        return pages\n",
    "    \n",
    "    \n",
    "    \n",
    "def process_text_analysis(bucket, document):\n",
    "    #Get the document from S3\n",
    "    s3_connection = boto3.resource(\"s3\")\n",
    "    \n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    result = client.get_object(Bucket=bucket, Key=document)\n",
    "    text = result['Body'].read().decode('utf-8')\n",
    "    res = json.loads(text)\n",
    "    \n",
    "    left_cor = []\n",
    "    top_cor = []\n",
    "    width_cor = []\n",
    "    height_cor = []\n",
    "    page = []\n",
    "\n",
    "    line_text = []\n",
    "\n",
    "    for response in res:\n",
    "        blocks=response[\"Blocks\"]\n",
    "        for block in blocks:\n",
    "            if (block[\"BlockType\"] == \"WORD\"):\n",
    "                left_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Left\"])))\n",
    "                top_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Top\"])))\n",
    "                width_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Width\"])))\n",
    "                height_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Height\"])))\n",
    "                line_text.append((block[\"Text\"]))\n",
    "                page.append(block[\"Page\"])\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(left_cor,top_cor,width_cor,height_cor,line_text,page)),columns = [\"xmin\",\"ymin\",\"width_cor\",\"height_cor\",\"line_text\",\"page\"])    \n",
    "    df[\"xmax\"] = (df[\"xmin\"] + df[\"width_cor\"])\n",
    "    df[\"ymax\"] = (df[\"ymin\"] + df[\"height_cor\"])\n",
    "    \n",
    "    pages = df.page.unique().tolist()\n",
    "    text_dict = {}\n",
    "    for p in pages:\n",
    "        dfp = df[df.page==p]\n",
    "        txt_list = dfp.line_text.tolist()\n",
    "        txt = \" \".join(txt_list)\n",
    "        text_dict[p] = txt\n",
    "\n",
    "    return df,text_dict,res\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--key-path\", type=str)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(\"Received arguments {}\".format(args))\n",
    "    print(\"Key Path:---\",args.key_path)\n",
    "    \n",
    "\n",
    "    key_path = args.key_path\n",
    "    key = key_path\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket ='lossadjustmentdataset'\n",
    "    dest_bucket = 'deeliptutorial'\n",
    "    s3_dir_key = 'XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut'\n",
    "\n",
    "    analyzer = DocumentProcessor()\n",
    "    digest = key.replace(\".pdf\",\"\")\n",
    "    analyzer.main(bucket,key_path,digest, dest_bucket, s3_dir_key, process_type=\"ANALYSIS\")\n",
    "\n",
    "\n",
    "    #  Extracting Daata path from s3\n",
    "\n",
    "\n",
    "    bucket_name='deeliptutorial'\n",
    "    prefix = 'XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut'\n",
    "\n",
    "    import boto3\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name,Prefix=prefix)\n",
    "    ct = 0\n",
    "    f_list = []\n",
    "    for page in pages:\n",
    "        for obj in page['Contents']:\n",
    "            if \".json\" in obj['Key'].lower():\n",
    "                ct = ct + 1\n",
    "                f_list.append(obj['Key'])\n",
    "                \n",
    "                \n",
    "    key_path = f_list[0]\n",
    "    bucket_name='deeliptutorial'\n",
    "    df,text_dict,res = process_text_analysis(bucket_name, key_path)\n",
    "    \n",
    "    ## Saving the file in op/ml/processing/train\n",
    "    \n",
    "    train_features_output_path = os.path.join(\"/opt/ml/processing/train\", \"output_sklearn_built_in.csv\")\n",
    "    print(\"Saving training features to {}\".format(train_features_output_path))\n",
    "    df.to_csv(train_features_output_path, header=False, index=False)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f8a783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2024-04-04-14-14-57-389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................\u001b[34mReceived arguments Namespace(key_path='documents/B0180ME1402570AA_06.pdf')\u001b[0m\n",
      "\u001b[34mKey Path:--- documents/B0180ME1402570AA_06.pdf\u001b[0m\n",
      "\u001b[34mProcessing type: Analysis\u001b[0m\n",
      "\u001b[34mStart Job Id: 55b84da82925bfd0393fd13b8e1268ac0eb09b47536f60ccb743ab2a4eee4adc\u001b[0m\n",
      "\u001b[34mMatching Job Found:55b84da82925bfd0393fd13b8e1268ac0eb09b47536f60ccb743ab2a4eee4adc\u001b[0m\n",
      "\u001b[34mstoring in S3\u001b[0m\n",
      "\u001b[34mregistering in s3 bucket...\u001b[0m\n",
      "\u001b[34mfile XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut/B0180ME1402570AA_06.json registered successfully!\u001b[0m\n",
      "\u001b[34mDone!\u001b[0m\n",
      "\u001b[34mSaving training features to /opt/ml/processing/train/output_sklearn_built_in.csv\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"preprocessing.py\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", source=\"/opt/ml/processing/train\"),\n",
    "    ],\n",
    "    arguments=[\"--key-path\", \"documents/B0180ME1402570AA_06.pdf\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431536d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34493af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
