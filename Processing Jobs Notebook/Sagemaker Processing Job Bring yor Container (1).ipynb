{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4801c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘docker’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dee098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "\n",
    "FROM python:3.10-slim-buster\n",
    "\n",
    "RUN pip3 install pandas==1.5.3 xlsxwriter==3.2.0 boto3==1.34.69\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8a37936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/4 : FROM python:3.10-slim-buster\n",
      " ---> 93b9055430ce\n",
      "Step 2/4 : RUN pip3 install pandas==1.5.3 xlsxwriter==3.2.0 boto3==1.34.69\n",
      " ---> Running in acf79cb3845e\n",
      "Collecting pandas==1.5.3\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 81.7 MB/s eta 0:00:00\n",
      "Collecting xlsxwriter==3.2.0\n",
      "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 159.9/159.9 kB 22.2 MB/s eta 0:00:00\n",
      "Collecting boto3==1.34.69\n",
      "  Downloading boto3-1.34.69-py3-none-any.whl (139 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.3/139.3 kB 36.8 MB/s eta 0:00:00\n",
      "Collecting python-dateutil>=2.8.1\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 50.1 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.21.0\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 16.3 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 505.5/505.5 kB 74.4 MB/s eta 0:00:00\n",
      "Collecting s3transfer<0.11.0,>=0.10.0\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.2/82.2 kB 21.6 MB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting botocore<1.35.0,>=1.34.69\n",
      "  Downloading botocore-1.34.77-py3-none-any.whl (12.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 95.9 MB/s eta 0:00:00\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.1/121.1 kB 23.4 MB/s eta 0:00:00\n",
      "Collecting six>=1.5\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, xlsxwriter, urllib3, six, numpy, jmespath, python-dateutil, pandas, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.34.69 botocore-1.34.77 jmespath-1.0.1 numpy-1.26.4 pandas-1.5.3 python-dateutil-2.9.0.post0 pytz-2024.1 s3transfer-0.10.1 six-1.16.0 urllib3-2.2.1 xlsxwriter-3.2.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container acf79cb3845e\n",
      " ---> 3c17c8e85b42\n",
      "Step 3/4 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Running in cbfd074bad45\n",
      "Removing intermediate container cbfd074bad45\n",
      " ---> e90fa79f7ea5\n",
      "Step 4/4 : ENTRYPOINT [\"python3\"]\n",
      " ---> Running in 45877aa58539\n",
      "Removing intermediate container 45877aa58539\n",
      " ---> 19844eccd7c3\n",
      "Successfully built 19844eccd7c3\n",
      "Successfully tagged sagemaker-processing-container:latest\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'sagemaker-processing-container' already exists in the registry with id '661082688832'\n",
      "The push refers to repository [661082688832.dkr.ecr.eu-west-2.amazonaws.com/sagemaker-processing-container]\n",
      "\n",
      "\u001b[1B44973bdb: Preparing \n",
      "\u001b[1B1f7f53ff: Preparing \n",
      "\u001b[1B1b185b95: Preparing \n",
      "\u001b[1Bedba7dbd: Preparing \n",
      "\u001b[1B55769c5e: Preparing \n",
      "\u001b[6B44973bdb: Pushed   210.1MB/203.1MBA\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2Klatest: digest: sha256:15d2b4283e19d99145da3e3f5c0b5e31fd2f2baaacd94ae67170f6431b47a01a size: 1582\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "ecr_repository = \"sagemaker-processing-container\"\n",
    "tag = \":latest\"\n",
    "\n",
    "uri_suffix = \"amazonaws.com\"\n",
    "if region in [\"cn-north-1\", \"cn-northwest-1\"]:\n",
    "    uri_suffix = \"amazonaws.com.cn\"\n",
    "processing_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "    account_id, region, uri_suffix, ecr_repository + tag\n",
    ")\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee338f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import urllib.parse\n",
    "import boto3\n",
    "import re\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import hashlib\n",
    "from botocore.errorfactory import ClientError\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import argparse\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "config = Config(retries = dict(max_attempts = 20),region_name='eu-west-2') # Amazon Textract client \n",
    "\n",
    "\n",
    "\n",
    "class ProcessType:\n",
    "    DETECTION = 1\n",
    "    ANALYSIS = 2\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    jobId = ''\n",
    "    textract = boto3.client('textract', config=config)\n",
    "    sqs = boto3.client('sqs',config=config)\n",
    "    sns = boto3.client('sns',config=config)\n",
    "\n",
    "    roleArn = ''\n",
    "    bucket = ''\n",
    "    document = ''\n",
    "\n",
    "    sqsQueueUrl = ''\n",
    "    snsTopicArn = ''\n",
    "    processType = ''\n",
    "    s3_dir_key = ''\n",
    "    dest_bucket = ''\n",
    "    \n",
    "    def main(self, bucketName, documentName, key_id, dest_bucket, s3_dir_key, process_type=\"DETECTION\"):\n",
    "        self.roleArn = 'arn:aws:iam::661082688832:role/service-role/AmazonSageMaker-ExecutionRole-20210921T210509'\n",
    "\n",
    "        self.bucket = bucketName\n",
    "        self.document = documentName\n",
    "        self.key_id = key_id\n",
    "        self.s3_dir_key = s3_dir_key\n",
    "        self.dest_bucket = dest_bucket\n",
    "        # self.file_name = file_name\n",
    "\n",
    "        #self.CreateTopicandQueue()\n",
    "        if process_type==\"DETECTION\":\n",
    "            self.ProcessDocument(ProcessType.DETECTION)\n",
    "        elif process_type==\"ANALYSIS\":\n",
    "            self.ProcessDocument(ProcessType.ANALYSIS)\n",
    "        else:\n",
    "            raise Exception(f\"process_type can be DETECTION/ANALYSIS, but \\\"{process_type}\\\" was passed.\")\n",
    "        #self.DeleteTopicandQueue()\n",
    "\n",
    "    def ProcessDocument(self, type):\n",
    "        jobFound = False\n",
    "\n",
    "        self.processType = type\n",
    "        validType = False\n",
    "\n",
    "        # Determine which type of processing to perform\n",
    "        if self.processType == ProcessType.DETECTION:\n",
    "            response = self.textract.start_document_text_detection(DocumentLocation={'S3Object': {'Bucket': self.bucket, 'Name': self.document}})\n",
    "            print('Processing type: Detection')\n",
    "            validType = True\n",
    "\n",
    "        if self.processType == ProcessType.ANALYSIS:\n",
    "            response = self.textract.start_document_analysis(DocumentLocation={'S3Object': {'Bucket': self.bucket, 'Name': self.document}},\n",
    "                                                             FeatureTypes=[\n",
    "                                                                 \"FORMS\",\"LAYOUT\"])\n",
    "            print('Processing type: Analysis')\n",
    "            validType = True\n",
    "\n",
    "        if validType == False:\n",
    "            raise Exception(f\"process_type can be DETECTION/ANALYSIS, but \\\"{process_type}\\\" was passed.\")\n",
    "\n",
    "        print('Start Job Id: ' + response['JobId'])\n",
    "        \n",
    "        \n",
    "        if self.processType == ProcessType.DETECTION:\n",
    "            while(self.textract.get_document_text_detection(JobId=str(response['JobId']))[\"JobStatus\"]!=\"SUCCEEDED\"):\n",
    "                pass\n",
    "        \n",
    "        if self.processType == ProcessType.ANALYSIS:\n",
    "            while(self.textract.get_document_analysis(JobId=str(response['JobId']))[\"JobStatus\"]!=\"SUCCEEDED\"):\n",
    "                pass\n",
    "        \n",
    "        print('Matching Job Found:' + response['JobId'])\n",
    "        print(\"storing in S3\")\n",
    "        jobFound = True\n",
    "        results = self.GetResults(response['JobId'])\n",
    "        self.StoreInS3(results)\n",
    "\n",
    "\n",
    "        print('Done!')\n",
    "\n",
    "    # Store the result in a S3 bucket\n",
    "    def StoreInS3(self, response):\n",
    "        print('registering in s3 bucket...')\n",
    "        outputInJsonText = str(response)\n",
    "        filename = str(self.key_id).split('/')[-1]\n",
    "        pdfTextExtractionS3ObjectName = os.path.join(self.s3_dir_key, str(filename) + \".json\") \n",
    "        pdfTextExtractionS3Bucket = self.dest_bucket\n",
    "\n",
    "        s3 = boto3.client('s3')\n",
    "\n",
    "        s3.put_object(Body=outputInJsonText,\n",
    "                      Bucket= pdfTextExtractionS3Bucket, Key=pdfTextExtractionS3ObjectName)\n",
    "        print('file ' + pdfTextExtractionS3ObjectName + ' registered successfully!')\n",
    "\n",
    "    def CreateTopicandQueue(self):\n",
    "\n",
    "        millis = str(int(round(time.time() * 1000)))\n",
    "\n",
    "        # Create SNS topic\n",
    "        snsTopicName = \"AmazonTextractTopic\" + millis\n",
    "\n",
    "        topicResponse = self.sns.create_topic(Name=snsTopicName)\n",
    "        self.snsTopicArn = topicResponse['TopicArn']\n",
    "\n",
    "        # create SQS queue\n",
    "        sqsQueueName = \"AmazonTextractQueue\" + millis\n",
    "        self.sqs.create_queue(QueueName=sqsQueueName)\n",
    "        self.sqsQueueUrl = self.sqs.get_queue_url(\n",
    "            QueueName=sqsQueueName)['QueueUrl']\n",
    "\n",
    "        attribs = self.sqs.get_queue_attributes(QueueUrl=self.sqsQueueUrl,\n",
    "                                                AttributeNames=['QueueArn'])['Attributes']\n",
    "\n",
    "        sqsQueueArn = attribs['QueueArn']\n",
    "\n",
    "        # Subscribe SQS queue to SNS topic\n",
    "        self.sns.subscribe(\n",
    "            TopicArn=self.snsTopicArn,\n",
    "            Protocol='sqs',\n",
    "            Endpoint=sqsQueueArn)\n",
    "\n",
    "        # Authorize SNS to write SQS queue\n",
    "        policy = \"\"\"{{\n",
    "  \"Version\":\"2012-10-17\",\n",
    "  \"Statement\":[\n",
    "    {{\n",
    "      \"Sid\":\"MyPolicy\",\n",
    "      \"Effect\":\"Allow\",\n",
    "      \"Principal\" : {{\"AWS\" : \"*\"}},\n",
    "      \"Action\":\"SQS:SendMessage\",\n",
    "      \"Resource\": \"{}\",\n",
    "      \"Condition\":{{\n",
    "        \"ArnEquals\":{{\n",
    "          \"aws:SourceArn\": \"{}\"\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "  ]\n",
    "}}\"\"\".format(sqsQueueArn, self.snsTopicArn)\n",
    "\n",
    "        response = self.sqs.set_queue_attributes(\n",
    "            QueueUrl=self.sqsQueueUrl,\n",
    "            Attributes={\n",
    "                'Policy': policy\n",
    "            })\n",
    "\n",
    "    def DeleteTopicandQueue(self):\n",
    "        self.sqs.delete_queue(QueueUrl=self.sqsQueueUrl)\n",
    "        self.sns.delete_topic(TopicArn=self.snsTopicArn)\n",
    "\n",
    "    def GetResults(self, jobId):\n",
    "        maxResults = 1000\n",
    "        paginationToken = None\n",
    "        finished = False\n",
    "        pages = []\n",
    "\n",
    "        while finished == False:\n",
    "\n",
    "            response = None\n",
    "\n",
    "            if self.processType == ProcessType.DETECTION:\n",
    "                if paginationToken == None:\n",
    "                    response = self.textract.get_document_text_detection(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults)\n",
    "                else:\n",
    "                    response = self.textract.get_document_text_detection(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults,\n",
    "                                                                         NextToken=paginationToken)\n",
    "                    \n",
    "                    \n",
    "            if self.processType == ProcessType.ANALYSIS:\n",
    "                if paginationToken == None:\n",
    "                    response = self.textract.get_document_analysis(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults)\n",
    "                else:\n",
    "                    response = self.textract.get_document_analysis(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults,\n",
    "                                                                         NextToken=paginationToken)\n",
    "\n",
    "\n",
    "            # Put response on pages list\n",
    "            pages.append(response)\n",
    "\n",
    "            if 'NextToken' in response:\n",
    "                paginationToken = response['NextToken']\n",
    "            else:\n",
    "                finished = True\n",
    "\n",
    "        # convert pages as JSON pattern\n",
    "        line_list=[]\n",
    "        word_counter = 0\n",
    "        line_counter = 0\n",
    "        n_pages = (pages[0][\"DocumentMetadata\"][\"Pages\"])\n",
    "  \n",
    "        for item in pages[0][\"Blocks\"]:\n",
    "            if item[\"BlockType\"] == \"LINE\":\n",
    "                line_counter = line_counter + 1\n",
    "                line_list.append(item[\"Text\"])\n",
    "            if item[\"BlockType\"] == \"WORD\":\n",
    "                word_counter = word_counter + 1\n",
    "                \n",
    "        rawtext=' '.join(line_list)\n",
    "        cutoff = min(500,len(rawtext))\n",
    "        language = \"EN\"\n",
    "        # response = clientt.detect_dominant_language(Text=str(rawtext)[:cutoff])\n",
    "        # language = response[\"Languages\"][0][\"LanguageCode\"]\n",
    "\n",
    "        pages = json.dumps(pages)\n",
    "        return pages\n",
    "    \n",
    "    \n",
    "    \n",
    "def process_text_analysis(bucket, document):\n",
    "    #Get the document from S3\n",
    "    s3_connection = boto3.resource(\"s3\")\n",
    "    \n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    result = client.get_object(Bucket=bucket, Key=document)\n",
    "    text = result['Body'].read().decode('utf-8')\n",
    "    res = json.loads(text)\n",
    "    \n",
    "    left_cor = []\n",
    "    top_cor = []\n",
    "    width_cor = []\n",
    "    height_cor = []\n",
    "    page = []\n",
    "\n",
    "    line_text = []\n",
    "\n",
    "    for response in res:\n",
    "        blocks=response[\"Blocks\"]\n",
    "        for block in blocks:\n",
    "            if (block[\"BlockType\"] == \"WORD\"):\n",
    "                left_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Left\"])))\n",
    "                top_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Top\"])))\n",
    "                width_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Width\"])))\n",
    "                height_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Height\"])))\n",
    "                line_text.append((block[\"Text\"]))\n",
    "                page.append(block[\"Page\"])\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(left_cor,top_cor,width_cor,height_cor,line_text,page)),columns = [\"xmin\",\"ymin\",\"width_cor\",\"height_cor\",\"line_text\",\"page\"])    \n",
    "    df[\"xmax\"] = (df[\"xmin\"] + df[\"width_cor\"])\n",
    "    df[\"ymax\"] = (df[\"ymin\"] + df[\"height_cor\"])\n",
    "    \n",
    "    pages = df.page.unique().tolist()\n",
    "    text_dict = {}\n",
    "    for p in pages:\n",
    "        dfp = df[df.page==p]\n",
    "        txt_list = dfp.line_text.tolist()\n",
    "        txt = \" \".join(txt_list)\n",
    "        text_dict[p] = txt\n",
    "\n",
    "    return df,text_dict,res\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--key-path\", type=str)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(\"Received arguments {}\".format(args))\n",
    "    print(\"Key Path:---\",args.key_path)\n",
    "    \n",
    "\n",
    "    key_path = args.key_path\n",
    "    key = key_path\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket ='lossadjustmentdataset'\n",
    "    dest_bucket = 'deeliptutorial'\n",
    "    s3_dir_key = 'XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut'\n",
    "\n",
    "    analyzer = DocumentProcessor()\n",
    "    digest = key.replace(\".pdf\",\"\")\n",
    "    analyzer.main(bucket,key_path,digest, dest_bucket, s3_dir_key, process_type=\"ANALYSIS\")\n",
    "\n",
    "\n",
    "    #  Extracting Daata path from s3\n",
    "\n",
    "\n",
    "    bucket_name='deeliptutorial'\n",
    "    prefix = 'XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut'\n",
    "\n",
    "    import boto3\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name,Prefix=prefix)\n",
    "    ct = 0\n",
    "    f_list = []\n",
    "    for page in pages:\n",
    "        for obj in page['Contents']:\n",
    "            if \".json\" in obj['Key'].lower():\n",
    "                ct = ct + 1\n",
    "                f_list.append(obj['Key'])\n",
    "                \n",
    "                \n",
    "    key_path = f_list[0]\n",
    "    bucket_name='deeliptutorial'\n",
    "    df,text_dict,res = process_text_analysis(bucket_name, key_path)\n",
    "    \n",
    "    ## Saving the file in op/ml/processing/train\n",
    "    \n",
    "    train_features_output_path = os.path.join(\"/opt/ml/processing/output\", \"output_container.csv\")\n",
    "    print(\"Saving training features to {}\".format(train_features_output_path))\n",
    "    df.to_csv(train_features_output_path, header=False, index=False)\n",
    "    \n",
    "    excel_path = key_path.replace(\"json\",\"xlsx\")\n",
    "    \n",
    "    with io.BytesIO() as output:\n",
    "        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "            df.to_excel(writer)\n",
    "        data = output.getvalue()\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket_name).put_object(Key=excel_path, Body=data)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf03d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "script_processor = ScriptProcessor(\n",
    "    command=[\"python3\"],\n",
    "    image_uri=processing_repository_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d7b5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sagemaker-processing-container-2024-04-04-17-58-07-960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................\u001b[34mReceived arguments Namespace(key_path='documents/B0180ME1402570AA_06.pdf')\u001b[0m\n",
      "\u001b[34mKey Path:--- documents/B0180ME1402570AA_06.pdf\u001b[0m\n",
      "\u001b[34mProcessing type: Analysis\u001b[0m\n",
      "\u001b[34mStart Job Id: 3f382f1a028e909d19c13d357ca5fb881a12de6299dfe92bda19a0683e9600ca\u001b[0m\n",
      "\u001b[34mMatching Job Found:3f382f1a028e909d19c13d357ca5fb881a12de6299dfe92bda19a0683e9600ca\u001b[0m\n",
      "\u001b[34mstoring in S3\u001b[0m\n",
      "\u001b[34mregistering in s3 bucket...\u001b[0m\n",
      "\u001b[34mfile XAAS_Practice/XAAS_processing_job_testing/preprocessing_outptut/B0180ME1402570AA_06.json registered successfully!\u001b[0m\n",
      "\u001b[34mDone!\u001b[0m\n",
      "\u001b[34mSaving training features to /opt/ml/processing/output/output_container.csv\u001b[0m\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-2-661082688832/sagemaker-processing-container-2024-04-04-17-58-07-960/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train_data', 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-2-661082688832/sagemaker-processing-container-2024-04-04-17-58-07-960/output/train_data', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}]}, 'ProcessingJobName': 'sagemaker-processing-container-2024-04-04-17-58-07-960', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '661082688832.dkr.ecr.eu-west-2.amazonaws.com/sagemaker-processing-container:latest', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocessing.py'], 'ContainerArguments': ['--key-path', 'documents/B0180ME1402570AA_06.pdf']}, 'RoleArn': 'arn:aws:iam::661082688832:role/service-role/parastest-role-4l0z5x30', 'ProcessingJobArn': 'arn:aws:sagemaker:eu-west-2:661082688832:processing-job/sagemaker-processing-container-2024-04-04-17-58-07-960', 'ProcessingJobStatus': 'Completed', 'ProcessingEndTime': datetime.datetime(2024, 4, 4, 18, 2, 33, 364000, tzinfo=tzlocal()), 'ProcessingStartTime': datetime.datetime(2024, 4, 4, 18, 1, 46, 377000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2024, 4, 4, 18, 2, 33, 699000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2024, 4, 4, 17, 58, 8, 470000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '220ed4ef-2497-4aff-98dd-90c2b9cc9f7f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '220ed4ef-2497-4aff-98dd-90c2b9cc9f7f', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1548', 'date': 'Thu, 04 Apr 2024 18:03:00 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "script_processor.run(\n",
    "    code=\"preprocessing.py\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", source=\"/opt/ml/processing/output\"),\n",
    "    ],\n",
    "    arguments=[\"--key-path\", \"documents/B0180ME1402570AA_06.pdf\"],\n",
    ")\n",
    "script_processor_job_description = script_processor.jobs[-1].describe()\n",
    "print(script_processor_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7967aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f205127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb88a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a982a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
