{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03b24e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from botocore.client import Config\n",
    "import io\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "config = Config(retries = dict(max_attempts = 20),region_name='eu-west-2') # Amazon Textract client \n",
    "\n",
    "\n",
    "\n",
    "class ProcessType:\n",
    "    DETECTION = 1\n",
    "    ANALYSIS = 2\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    jobId = ''\n",
    "    textract = boto3.client('textract', config=config)\n",
    "    sqs = boto3.client('sqs',config=config)\n",
    "    sns = boto3.client('sns',config=config)\n",
    "\n",
    "    roleArn = ''\n",
    "    bucket = ''\n",
    "    document = ''\n",
    "\n",
    "    sqsQueueUrl = ''\n",
    "    snsTopicArn = ''\n",
    "    processType = ''\n",
    "    s3_dir_key = ''\n",
    "    dest_bucket = ''\n",
    "    \n",
    "    def main(self, bucketName, documentName, key_id, dest_bucket, s3_dir_key, process_type=\"DETECTION\"):\n",
    "        self.roleArn = 'arn:aws:iam::661082688832:role/service-role/AmazonSageMaker-ExecutionRole-20210921T210509'\n",
    "\n",
    "        self.bucket = bucketName\n",
    "        self.document = documentName\n",
    "        self.key_id = key_id\n",
    "        self.s3_dir_key = s3_dir_key\n",
    "        self.dest_bucket = dest_bucket\n",
    "        # self.file_name = file_name\n",
    "\n",
    "        #self.CreateTopicandQueue()\n",
    "        if process_type==\"DETECTION\":\n",
    "            self.ProcessDocument(ProcessType.DETECTION)\n",
    "        elif process_type==\"ANALYSIS\":\n",
    "            self.ProcessDocument(ProcessType.ANALYSIS)\n",
    "        else:\n",
    "            raise Exception(f\"process_type can be DETECTION/ANALYSIS, but \\\"{process_type}\\\" was passed.\")\n",
    "        #self.DeleteTopicandQueue()\n",
    "\n",
    "    def ProcessDocument(self, type):\n",
    "        jobFound = False\n",
    "\n",
    "        self.processType = type\n",
    "        validType = False\n",
    "\n",
    "        # Determine which type of processing to perform\n",
    "        if self.processType == ProcessType.DETECTION:\n",
    "            response = self.textract.start_document_text_detection(DocumentLocation={'S3Object': {'Bucket': self.bucket, 'Name': self.document}})\n",
    "            print('Processing type: Detection')\n",
    "            validType = True\n",
    "\n",
    "        if self.processType == ProcessType.ANALYSIS:\n",
    "            response = self.textract.start_document_analysis(DocumentLocation={'S3Object': {'Bucket': self.bucket, 'Name': self.document}},\n",
    "                                                             FeatureTypes=[\n",
    "                                                                 \"FORMS\",\"LAYOUT\"])\n",
    "            print('Processing type: Analysis')\n",
    "            validType = True\n",
    "\n",
    "        if validType == False:\n",
    "            raise Exception(f\"process_type can be DETECTION/ANALYSIS, but \\\"{process_type}\\\" was passed.\")\n",
    "\n",
    "        print('Start Job Id: ' + response['JobId'])\n",
    "        \n",
    "        \n",
    "        if self.processType == ProcessType.DETECTION:\n",
    "            while(self.textract.get_document_text_detection(JobId=str(response['JobId']))[\"JobStatus\"]!=\"SUCCEEDED\"):\n",
    "                pass\n",
    "        \n",
    "        if self.processType == ProcessType.ANALYSIS:\n",
    "            while(self.textract.get_document_analysis(JobId=str(response['JobId']))[\"JobStatus\"]!=\"SUCCEEDED\"):\n",
    "                pass\n",
    "        \n",
    "        print('Matching Job Found:' + response['JobId'])\n",
    "        print(\"storing in S3\")\n",
    "        jobFound = True\n",
    "        results = self.GetResults(response['JobId'])\n",
    "        self.StoreInS3(results)\n",
    "\n",
    "\n",
    "        print('Done!')\n",
    "\n",
    "    # Store the result in a S3 bucket\n",
    "    def StoreInS3(self, response):\n",
    "        print('registering in s3 bucket...')\n",
    "        outputInJsonText = str(response)\n",
    "        filename = str(self.key_id).split('/')[-1]\n",
    "        pdfTextExtractionS3ObjectName = os.path.join(self.s3_dir_key, str(filename) + \".json\") \n",
    "        pdfTextExtractionS3Bucket = self.dest_bucket\n",
    "\n",
    "        s3 = boto3.client('s3')\n",
    "\n",
    "        s3.put_object(Body=outputInJsonText,\n",
    "                      Bucket= pdfTextExtractionS3Bucket, Key=pdfTextExtractionS3ObjectName)\n",
    "        print('file ' + pdfTextExtractionS3ObjectName + ' registered successfully!')\n",
    "\n",
    "    def CreateTopicandQueue(self):\n",
    "\n",
    "        millis = str(int(round(time.time() * 1000)))\n",
    "\n",
    "        # Create SNS topic\n",
    "        snsTopicName = \"AmazonTextractTopic\" + millis\n",
    "\n",
    "        topicResponse = self.sns.create_topic(Name=snsTopicName)\n",
    "        self.snsTopicArn = topicResponse['TopicArn']\n",
    "\n",
    "        # create SQS queue\n",
    "        sqsQueueName = \"AmazonTextractQueue\" + millis\n",
    "        self.sqs.create_queue(QueueName=sqsQueueName)\n",
    "        self.sqsQueueUrl = self.sqs.get_queue_url(\n",
    "            QueueName=sqsQueueName)['QueueUrl']\n",
    "\n",
    "        attribs = self.sqs.get_queue_attributes(QueueUrl=self.sqsQueueUrl,\n",
    "                                                AttributeNames=['QueueArn'])['Attributes']\n",
    "\n",
    "        sqsQueueArn = attribs['QueueArn']\n",
    "\n",
    "        # Subscribe SQS queue to SNS topic\n",
    "        self.sns.subscribe(\n",
    "            TopicArn=self.snsTopicArn,\n",
    "            Protocol='sqs',\n",
    "            Endpoint=sqsQueueArn)\n",
    "\n",
    "        # Authorize SNS to write SQS queue\n",
    "        policy = \"\"\"{{\n",
    "  \"Version\":\"2012-10-17\",\n",
    "  \"Statement\":[\n",
    "    {{\n",
    "      \"Sid\":\"MyPolicy\",\n",
    "      \"Effect\":\"Allow\",\n",
    "      \"Principal\" : {{\"AWS\" : \"*\"}},\n",
    "      \"Action\":\"SQS:SendMessage\",\n",
    "      \"Resource\": \"{}\",\n",
    "      \"Condition\":{{\n",
    "        \"ArnEquals\":{{\n",
    "          \"aws:SourceArn\": \"{}\"\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "  ]\n",
    "}}\"\"\".format(sqsQueueArn, self.snsTopicArn)\n",
    "\n",
    "        response = self.sqs.set_queue_attributes(\n",
    "            QueueUrl=self.sqsQueueUrl,\n",
    "            Attributes={\n",
    "                'Policy': policy\n",
    "            })\n",
    "\n",
    "    def DeleteTopicandQueue(self):\n",
    "        self.sqs.delete_queue(QueueUrl=self.sqsQueueUrl)\n",
    "        self.sns.delete_topic(TopicArn=self.snsTopicArn)\n",
    "\n",
    "    def GetResults(self, jobId):\n",
    "        maxResults = 1000\n",
    "        paginationToken = None\n",
    "        finished = False\n",
    "        pages = []\n",
    "\n",
    "        while finished == False:\n",
    "\n",
    "            response = None\n",
    "\n",
    "            if self.processType == ProcessType.DETECTION:\n",
    "                if paginationToken == None:\n",
    "                    response = self.textract.get_document_text_detection(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults)\n",
    "                else:\n",
    "                    response = self.textract.get_document_text_detection(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults,\n",
    "                                                                         NextToken=paginationToken)\n",
    "                    \n",
    "                    \n",
    "            if self.processType == ProcessType.ANALYSIS:\n",
    "                if paginationToken == None:\n",
    "                    response = self.textract.get_document_analysis(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults)\n",
    "                else:\n",
    "                    response = self.textract.get_document_analysis(JobId=jobId,\n",
    "                                                                         MaxResults=maxResults,\n",
    "                                                                         NextToken=paginationToken)\n",
    "\n",
    "\n",
    "            # Put response on pages list\n",
    "            pages.append(response)\n",
    "\n",
    "            if 'NextToken' in response:\n",
    "                paginationToken = response['NextToken']\n",
    "            else:\n",
    "                finished = True\n",
    "\n",
    "        # convert pages as JSON pattern\n",
    "        line_list=[]\n",
    "        word_counter = 0\n",
    "        line_counter = 0\n",
    "        n_pages = (pages[0][\"DocumentMetadata\"][\"Pages\"])\n",
    "  \n",
    "        for item in pages[0][\"Blocks\"]:\n",
    "            if item[\"BlockType\"] == \"LINE\":\n",
    "                line_counter = line_counter + 1\n",
    "                line_list.append(item[\"Text\"])\n",
    "            if item[\"BlockType\"] == \"WORD\":\n",
    "                word_counter = word_counter + 1\n",
    "                \n",
    "        rawtext=' '.join(line_list)\n",
    "        cutoff = min(500,len(rawtext))\n",
    "        language = \"EN\"\n",
    "        # response = clientt.detect_dominant_language(Text=str(rawtext)[:cutoff])\n",
    "        # language = response[\"Languages\"][0][\"LanguageCode\"]\n",
    "\n",
    "        pages = json.dumps(pages)\n",
    "        return pages\n",
    "\n",
    "    \n",
    "    \n",
    "def process_text_analysis(bucket, document):\n",
    "    #Get the document from S3\n",
    "    s3_connection = boto3.resource(\"s3\")\n",
    "    \n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    result = client.get_object(Bucket=bucket, Key=document)\n",
    "    text = result['Body'].read().decode('utf-8')\n",
    "    res = json.loads(text)\n",
    "    \n",
    "    left_cor = []\n",
    "    top_cor = []\n",
    "    width_cor = []\n",
    "    height_cor = []\n",
    "    page = []\n",
    "\n",
    "    line_text = []\n",
    "\n",
    "    for response in res:\n",
    "        blocks=response[\"Blocks\"]\n",
    "        for block in blocks:\n",
    "            if (block[\"BlockType\"] == \"WORD\"):\n",
    "                left_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Left\"])))\n",
    "                top_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Top\"])))\n",
    "                width_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Width\"])))\n",
    "                height_cor.append(float(\"{:.2f}\".format(block[\"Geometry\"][\"BoundingBox\"][\"Height\"])))\n",
    "                line_text.append((block[\"Text\"]))\n",
    "                page.append(block[\"Page\"])\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(left_cor,top_cor,width_cor,height_cor,line_text,page)),columns = [\"xmin\",\"ymin\",\"width_cor\",\"height_cor\",\"line_text\",\"page\"])    \n",
    "    df[\"xmax\"] = (df[\"xmin\"] + df[\"width_cor\"])\n",
    "    df[\"ymax\"] = (df[\"ymin\"] + df[\"height_cor\"])\n",
    "    \n",
    "    pages = df.page.unique().tolist()\n",
    "    text_dict = {}\n",
    "    for p in pages:\n",
    "        dfp = df[df.page==p]\n",
    "        txt_list = dfp.line_text.tolist()\n",
    "        txt = \" \".join(txt_list)\n",
    "        text_dict[p] = txt\n",
    "\n",
    "    return df,text_dict,res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08a43341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    bucket_name = event['Records'][0]['s3']['bucket']['name']\n",
    "    key = event['Records'][0]['s3']['object']['key']\n",
    "    \n",
    "    print(\"key{---}\",key)\n",
    "    print(\"bucket_name:----\",bucket_name)    \n",
    "#     s3_dir_key = 'XAAS_Practice/LambdaInputData/json_document'\n",
    "#     analyzer = DocumentProcessor()\n",
    "#     key_path = key\n",
    "#     digest = key.replace(\".pdf\",\"\")\n",
    "#     analyzer.main(bucket_name,key_path,digest, bucket_name, s3_dir_key, process_type=\"ANALYSIS\")\n",
    "    \n",
    "    prefix = 'XAAS_Practice/LambdaInputData/json_document'\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name,Prefix=prefix)\n",
    "    ct = 0\n",
    "    f_list = []\n",
    "    for page in pages:\n",
    "        for obj in page['Contents']:\n",
    "            if \".json\" in obj['Key'].lower():\n",
    "                ct = ct + 1\n",
    "                f_list.append(obj['Key'])\n",
    "                  \n",
    "                \n",
    "    key_path = f_list[0]\n",
    "    df,text_dict,res = process_text_analysis(bucket_name, key_path)\n",
    "    excel_path = key_path.replace(\".json\",\".xlsx\")\n",
    "    print(\"excel_path\",excel_path)\n",
    "    \n",
    "    with io.BytesIO() as output:\n",
    "        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "            df.to_excel(writer)\n",
    "        data = output.getvalue()\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket_name).put_object(Key=excel_path, Body=data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3d7d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {'Records': [{'eventVersion': '2.1', 'eventSource': 'aws:s3', 'awsRegion': 'eu-west-2', 'eventTime': '2024-04-01T07:26:35.788Z', 'eventName': 'ObjectCreated:Put', 'userIdentity': {'principalId': 'AWS:AROAZT24ZJVAAOLMJR6XS:Deelip166585@exlservice.com'}, 'requestParameters': {'sourceIPAddress': '163.116.195.118'}, 'responseElements': {'x-amz-request-id': 'Y6PSHYDBJ4P34JF5', 'x-amz-id-2': 'TsgdR+zyq39sa+Lf0mzrgsivLF837zJJFECXpFBdN0gvfdFgBqO+jK7Z9lqHZ+RjDigMsPigtfxtSarUkqVYye5V1FLGKfmWNSbfbsIyuCU='}, 's3': {'s3SchemaVersion': '1.0', 'configurationId': 'd05b9c0b-b683-4ffd-9bfa-a80fa81d58b5', 'bucket': {'name': 'deeliptutorial', 'ownerIdentity': {'principalId': 'A3R036IGOPVHI4'}, 'arn': 'arn:aws:s3:::deeliptutorial'}, 'object': {'key': 'XAAS_Practice/LambdaInputData/B0180ME1402570AA_06.pdf', 'size': 1181554, 'eTag': '994943182e73cd53ae9209d77a5bbef8', 'sequencer': '00660A61ABAABAFD86'}}}]}\n",
    "context = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89d2e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key{---} XAAS_Practice/LambdaInputData/B0180ME1402570AA_06.pdf\n",
      "bucket_name:---- deeliptutorial\n",
      "excel_path XAAS_Practice/LambdaInputData/json_document/B0180ME1402570AA_06.xlsx\n"
     ]
    }
   ],
   "source": [
    "lambda_handler(event, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf54f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98546db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
